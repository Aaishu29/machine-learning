{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKDskdynKrl3"
   },
   "source": [
    "# Assignment 3\n",
    "### Question 1 - non CNN models\n",
    "\n",
    "This section of the assignment focuses on implementing non - CNN methods on the dataset to classify it into its defined categories.\n",
    "\n",
    "The dataset is fashion MNIST with 786 columns and 60,000 observations. Due to the large size of data and incompatible system configuration, the implementation was moved to google colabs. This dataset is the representation of five different kinds of fashion items. Each row represents one fashion item and every column of the row stores the pixel value used to generate the image of the fashion item. Based on these values, we have to classify the data into 5 distinct categories.\n",
    " \n",
    "The different methods used for implementation and the reasons why, have been discussed along with the code snippets, combining all the questions asked in the assignment.\n",
    "\n",
    "Also, some sections had to interrupt when executing the second time due to long hours they were taking for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "YjxWZyPQ_E45",
    "outputId": "b1e3b5d8-0a6c-40c0-bfd1-cf8dc1d6ab09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Id  Label  1  2  3  4  5  6  7  8  ...  775  776  777  778  779  \\\n",
      "0      10000      4  0  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
      "1      10001      0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
      "2      10002      0  0  0  0  0  0  0  0  0  ...   31    9    0    0    0   \n",
      "3      10003      4  0  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
      "4      10004      1  0  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
      "...      ...    ... .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...   \n",
      "59995  69995      3  0  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
      "59996  69996      0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
      "59997  69997      4  0  0  0  0  0  0  0  0  ...   27    0    0    0    0   \n",
      "59998  69998      0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
      "59999  69999      2  0  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
      "\n",
      "       780  781  782  783  784  \n",
      "0        0    0    0    0    0  \n",
      "1        0    0    0    0    0  \n",
      "2        0    0    0    0    0  \n",
      "3        0    0    0    0    0  \n",
      "4        0    0    0    0    0  \n",
      "...    ...  ...  ...  ...  ...  \n",
      "59995    0    0    0    0    0  \n",
      "59996    0    0    0    0    0  \n",
      "59997    0    0    0    0    0  \n",
      "59998    0    0    0    0    0  \n",
      "59999    0    0    0    0    0  \n",
      "\n",
      "[60000 rows x 786 columns]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = pd.read_csv(\"train.csv\",header=0)\n",
    "test_data = pd.read_csv(\"testX.csv\", header=0)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ocbnfo2E2c8i"
   },
   "source": [
    "The implementation started with loading the train and test dataset. The test dataset had 10,000 observations on which the accuracy of the model was calculated. The train dataset was then split into train and validation set(10% of the total data), to check the validity of the model before running it on the test data. The data was transformed into a 28X28 matrix to generate the images for better visual understanding using imshow()(results in part b of the assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt7vt57hOmvz"
   },
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,2:].values #all columns except the last one\n",
    "y = dataset.iloc[:,1].values #only the last column\n",
    "X_test = test_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUN90MByNMbQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BgekZ9M15IqW"
   },
   "source": [
    "Once the train, validation and test sets were made, it was then preprocessed. For preprocessing, a subset of minmax preprocessing and zscore normalization was chosen. The major difference between the two is that the zscore centers the data by subtracting the mean and dividing by standard deviation(sets the overall mean as zero and standard deviation as one) whereas, minmax scaler rescales the data by setting minimum to 0 and maximum to 1(by default) and all the other values scale accordingly. The end goal is to maximize the variance such that dimensionality reduction could work better. For this, if we go by values itself, we would be able to distinguish by the fact that minmax ranges the data between 0 and 1 whereas standardscaler has ~68% data ranging between -1 and 1 and ~95% between-1.96 and 1.96, which shows that data is more widespread in standardscaler than in minmax.\n",
    "\n",
    "The values for each column range between 0 and 255. Since the distinguishing factor between the classes would be the pixel intensities at different points, therefore, scaling them using Standard Scaler would be better as it would evenly distribute the intensities making darker pixels darker and lighter more lighter. Also, minmax is more sensitive to noisy data, hence if there is noise in data then it would impact the over scaling of the set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fLFHrGd0zjUb"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_val = sc.transform(X_val)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veZdb3GweX1w"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "mm_scaler = preprocessing.MinMaxScaler()\n",
    "X_train = mm_scaler.fit_transform(X_train)\n",
    "X_val = mm_scaler.transform(X_val)\n",
    "X_test = mm_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GA1gSLTL5Pok"
   },
   "source": [
    "Once the data is preprocessed, the next step is to model the data. But due to the large size of the columns, the computational time and complexity increases to a great extent. To resolve this issue, the dimensionality of the data is reduced making sure that the information is not lost. To reduce the dimensionality, the following methods were taken into consideration(taking forward the methods described in assignment 2): \n",
    "1. PCA\n",
    "2. LDA\n",
    "3. Isomap\n",
    "4. LocallyLinearEmbedding\n",
    "5. SpectralEmbedding\n",
    "6. t-SNE. \n",
    "\n",
    "Out of all the methods for dimensionality reduction, t-SNE performed the best when the data had to be classified. But when talking about time complexity the best performance was given by PCA(least time), LDA and the other methods. When we bring time complexity as a factor for evaluation, then t-SNE should be rejected as it took ~8 hours to compute the model whereas PCA took ~321 seconds to generate the results(when n_componenets were not specified). Hence PCA was a better option than any other method. \n",
    "\n",
    "Also, when the graphs of the results were compared, t-SNE did not provide stability in classification. Once it was pretty much seperable whereas on second run PCA seemed to classify better.\n",
    "\n",
    "Also, only t-SNE was tested for that long. The other methods were interrupted inbetween because there performance had been tested during assignment 2 and on evaluation only t-SNE had seemed to outperform all of them in terms of classification as it preserves the local distances between the neighbours. When the dataset is smaller then, t-SNE could be considered but for cases such as this, time complexity plays an important role and cannot be ignored. Therfore, PCA seemed to be the best option available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ybLlc95jDspb",
    "outputId": "c26cdd36-d19a-4ff4-88c9-1c165c1557cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.85 s, sys: 316 ms, total: 4.17 s\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import PCA\n",
    "transformer = PCA(n_components=2,random_state=42) \n",
    "X_transformed_PCA = transformer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "O5AF8gdtEYnN",
    "outputId": "c63abc27-757d-48a6-d565-00056fde20f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.8 s, sys: 1.12 s, total: 25 s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "transformer = LDA(n_components=2) \n",
    "X_transformed_PCA = transformer.fit_transform(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ng07cySCFKuy"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from sklearn.manifold import Isomap \n",
    "embedding = Isomap(n_components=2) \n",
    "X_transformed_Isomap = embedding.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Xr06Wg5FSRJ"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from sklearn.manifold import LocallyLinearEmbedding \n",
    "embedding = LocallyLinearEmbedding(n_components=2,random_state=42) \n",
    "X_transformed_LocallyLinearEmbedding = embedding.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1o6w4joFaFW"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from sklearn.manifold import SpectralEmbedding \n",
    "embedding = SpectralEmbedding(n_components=2,random_state=42) \n",
    "X_transformed_SpectralEmbedding = embedding.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qk_979AfFiuE"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from sklearn.manifold import TSNE \n",
    "embedding = TSNE(n_components=2,random_state=42) \n",
    "X_transformed_TSNE = embedding.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "45__ryJMF0fq"
   },
   "source": [
    "Finally PCA was chosen over other methods, as can be observed from the runtimes of the methods. Most of them had to be interrupted because of large dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "olbzaM9xOqPK"
   },
   "outputs": [],
   "source": [
    "#applying PCA on the dataset\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(random_state=42,n_components=10)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_val = pca.transform(X_val)\n",
    "X_test = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pkXgFUVLGBZx"
   },
   "source": [
    "Once the data is processed and its dimensionality is reduced, then various methods of classification are tested to shortlist the ones that perform well on the dataset. To get quicker results, the number of components for PCA were set to 10 as it covers more than 70% of the variance explained by the model. Using this data, the number of columns are reduced to 10 thereby making computation simpler and quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6bOVt2nRPjQm"
   },
   "source": [
    "Logistic regression works on probability function checking whether the data belongs to a particular class or not. This turn out to not be a good method as it stopped beyond certain number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "IKuJvuvDOsas",
    "outputId": "9c8b203b-d635-49fc-c53f-6c65c0ebcf9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression classifier on training set: 0.64\n",
      "Accuracy of Logistic regression classifier on test set: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(logreg.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(logreg.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "66-01dklNgtA"
   },
   "source": [
    "Decision Tree classifier classifies the data into groups of data that can be categorised later. This model helps in finding patterns in data and categorizing them into leaves of different classes. The leaf branches define the properties that lead to the categorization. \n",
    "\n",
    "The decision tree model is very easy to overfit. As the depth of the tree increases, the patterns are further disintegrated into subsets which might work well for the train model but misclassify the validation/test model. More depth makes the model more prone to noise and hence the model becomes overfit. Extremely small values result in underfitting as well as it is unable to break down the model into desired number of classes.\n",
    "\n",
    "For testing phase, default values were considered. Even when the max_depth was altered the performance did not improve beyond 85% on the validation data at max_depth = 45. Beyond this, a decrease in accuracy was observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "kP-WcAIKOzFX",
    "outputId": "4d358574-4f79-48aa-ab42-3e9e023658b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 1.00\n",
      "Accuracy of Decision Tree classifier on test set: 0.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_JNidWANLdvK"
   },
   "source": [
    "The KNN model takes into consideration k nearest neighbours for each point to classify it into one of the defined groups. The class that has its maximum data elements near the point under consideration will have the point under its domain. To avoid tie, odd numbers are preferred but that is usually the case when there are two classes. The parameters that can be tunned for better performamce are k neighbors.\n",
    "\n",
    "Greater the K smoother the boundry( removal of noise) but higher the chances of misclassification(underfitting). Lower value of K can have a lot of noise that will have higher influence on the dataset(overfitting). Both of these lead to lower accuracy on the validation/test data. Another factor is the computational complexity, which is lower for smaller value of k and higher for higher value of K. \n",
    "\n",
    "For this model, we are choosing the default values set for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "20XTafvmOFF5",
    "outputId": "7645b005-f214-4cad-f1aa-5a641c4d430c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-NN classifier on training set: 0.90\n",
      "Accuracy of K-NN classifier on test set: 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6lTFumPMvIK"
   },
   "source": [
    "Support Vector Machine tries to find a hyperplane that correctly classifies the data into one of the defined groups. The goal of SVM is to maximize the distance between the hyperplanes making sure that it does not misclassifiy the data.  \n",
    "\n",
    "C value is the regularization parameter that determines the distance between the hyperplanes. If the value of C is small, then the distance between the hyperplanes will be larger. This will result in underfitting of the data and it may misclassify some data. If the value of C is larger, then the distance between the hyperplanes will be smaller. This will result in overfitting of the data as it will try to accommodate as many class datapoints as it could to avoid misclassification. This will result in high training accuracy but lower test accuracy.\n",
    "\n",
    "This model was tested for default values when comparing to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "avb4dzp1OfZX",
    "outputId": "968ec492-4f01-4e0f-f5b5-aa5081bd0e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on training set: 0.85\n",
      "Accuracy of SVM classifier on test set: 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "print('Accuracy of SVM classifier on training set: {:.2f}'\n",
    "     .format(svm.score(X_train, y_train)))\n",
    "print('Accuracy of SVM classifier on test set: {:.2f}'\n",
    "     .format(svm.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOd1a7_nxVUi"
   },
   "source": [
    "XGBoost is an improvised version of gradient Tree boosting and AdaGrad. It is basically a decision tree based ensemble that works on Gradient boosting framework. It has various additional functionalities when it comes to unstructured data. It improvises the time and computational power making non-CNN work better. Results show that it has outperformed Gradient boosting, Random forest and Logistic Regression. For xgboost, various parameters can be altered such as n_estimators, max_depth, learning rate etc. It is said that XGBoost performs better than any classification algorithm in terms of time complexity.\n",
    "\n",
    "It involves the process of improving the performance of the model by fiting the new tree on the old one. It updates the tree to make it more effective and accurate. This model is usually prone to overfitting if the number of trees are very larger. For smaller values, as the trees increase, the accuracy improves as well but beyond certain value it starts getting influenced by the noisy data and we observe a dip in the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MadvQPUYm9dZ",
    "outputId": "e159dae6-051a-4e9f-a55c-5f07d088fecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on training set: 0.81\n",
      "Accuracy of SVM classifier on test set: 0.80\n"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "import xgboost as xgb\n",
    "model=xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print('Accuracy of SVM classifier on training set: {:.2f}'\n",
    "     .format(model.score(X_train, y_train)))\n",
    "print('Accuracy of SVM classifier on test set: {:.2f}'\n",
    "     .format(model.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mye7SWTGQduT"
   },
   "source": [
    "From all the above methods, logistic regression was eliminated because of low accuracy. The others were hypertuned one by one for most accuaret results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxsGOW-6m_2L"
   },
   "source": [
    "Now the number of components for PCA were set to 150 so that they cover more than 90% of the variance explained by the model. The train, val and test generated were passed through the approved models to check for model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1j1hSxWnGiB"
   },
   "outputs": [],
   "source": [
    "#applying PCA on the dataset\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(random_state=42,n_components=150)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_val = pca.transform(X_val)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTEMiGIuqSQM"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k_list=[50,100,250,500,1000]\n",
    "for k in k_list:\n",
    "  knn = KNeighborsClassifier(n_neighbors=k)\n",
    "  knn.fit(X_train, y_train)\n",
    "  print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "      .format(knn.score(X_train, y_train)))\n",
    "  print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "      .format(knn.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9qoz0qbnFmU"
   },
   "source": [
    "When the values for KNN were hypertuned, the maximum accuracy that was obtained was 86.69% for test data. \n",
    "\n",
    "Lower value of k results in overfitting of the model because it is highly prone to noisy data and has less number of data elements to compare the datapoint with to correctly classify it to the right right class( misclassification of the data) But in our case the smallest value chosen is the optimal choice. Higher value of k results in underfitting the model because it has way too many data elements to comapre the datapoint with that the distinction between the nearest and not so near elements becomes blurry. This will again result in misclassification of data and in many cases may output classes that have dominance in the dataset. When choosing an optimal value of K, it should be larger enough to minimize the noise in the data and small enough to avoid misclassification of data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "IwACWZUh0zSP",
    "outputId": "1156a8b3-9494-4786-ce3f-efce43f84025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGBoost classifier on training set: 0.88\n",
      "667.6143534183502\n",
      "Accuracy of XGBoost classifier on test set: 0.86\n",
      "667.8802578449249\n"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "import xgboost as xgb\n",
    "import time\n",
    "model=xgb.XGBClassifier(eta=0.3, max_depth=40, seed=42, min_child_weight=300, objective='multi:softmax')\n",
    "start_time=time.time()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of XGBoost classifier on training set: {:.2f}'\n",
    "     .format(model.score(X_train, y_train)))\n",
    "print(time.time()-start_time)\n",
    "print('Accuracy of XGBoost classifier on test set: {:.2f}'\n",
    "     .format(model.score(X_val, y_val)))\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wV7dxgE48oqy"
   },
   "source": [
    "The accuracy obtained for XGBoost was lower than expected. With the advantages available, it could easily outperform the other algorithm. But there are so many parameters that need to be tuned as a result of which some combinations which perform well are left out. This combination had performed better than any of the others that have been tried so far. There are many more that can outperform these parameters. The general observations were as follows:\n",
    "1. Smaller values of eta resulted in more time consumption than required.\n",
    "2. Max_depth was tested for range 20-50, with better accuracy at 45 beyond which decrease in accuracy was observed\n",
    "3. n_estimators were checked between 100-500 with better results at 300 beyond which decrease in accuracy was observed\n",
    "4. objective was set to multi:softmax. \n",
    "\n",
    "Yet the results were nor any better than SVM but rather had similar observations. Even the time taken for computation was more for XGBoost. It was expected that the time taken would be least in this case. Even the accuracies were lower than SVM. Since SVM outperformed XGBoost, therefore, XGBoost was dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "daGkNYnfS5oZ",
    "outputId": "111a3103-795b-473c-849b-28aa78933ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on training set: 0.94\n",
      "580.9654021263123\n",
      "Accuracy of SVM classifier on test set: 0.89\n",
      "612.8806030750275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "gamma=[0.001,0.01,0.05,1]\n",
    "C=[1,5,10,100,500]\n",
    "# These were the combinations that were tested repeatedly. GridSearch was taking longer than expected, hence the results have not been included. \n",
    "# The code has been commented. It can be uncommented and tested to view the results.\n",
    "# parameters = {'gamma'=[0.001,0.01,0.05,1], 'C':[1,5,10,20,100]}\n",
    "# svc = svm.SVC()\n",
    "# clf = GridSearchCV(svc, parameters)\n",
    "# clf.fit(X_train, y_train)\n",
    "# The best outcome has been written below:\n",
    "svm = SVC(gamma=0.001, C = 10, kernel='rbf')\n",
    "import time\n",
    "start_time = time.time()\n",
    "svm.fit(X_train, y_train)\n",
    "print('Accuracy of SVM classifier on training set: {:.2f}'\n",
    "     .format(svm.score(X_train, y_train)))\n",
    "print(time.time()-start_time)\n",
    "print('Accuracy of SVM classifier on test set: {:.2f}'\n",
    "     .format(svm.score(X_val, y_val)))\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "go_Lv6MCAmpk"
   },
   "source": [
    "SVM has been able to outperform all the other classification models tested so far. The maximum accuracy reported on the test data is 89.2%(reported for 50% of the data). As defined above, the following parameters were hypertuned. The kernel was set to 'rbf' as linear took a lot of time to compile the results and the difference in the accuarcy of test result was also marginal. For the other parameters, the model performed the best at c=10 and gamma=0.001.\n",
    "\n",
    "The factors that determine which c value is the best are:\n",
    "1. to have hyperplanes that do not misclassify the data\n",
    "2. to have the largest minimum margin between hyperplanes\n",
    "\n",
    "C at 10 and 100 had minimal difference when the results were compared. Rather a slight dip in the accuracy for c=100 was observed. also considering the above statements, we can say that '10' best satisfies the purpose of the model as it would have similar misclassification as 100 and will have larger margin between the hyperplanes as compared to 100.\n",
    "\n",
    "Also, having lower curvature gave better results than higher gamma value which suggests that data is not wide spread, hence having lower curvature better performs on the results.\n",
    "\n",
    "The time recorded for calculating training accuracy and test accuarcy was better than XGBoost, therefore, we can say that SVM outperforms all the other models and their constraints that were tested on the dataset.\n",
    "\n",
    "The runtime for fitting the model and calculating the accuracy of training model was ~580 seconds.\n",
    "\n",
    "The accuracy obtained for training was maximum for c=10 and gamma =0.01 and was equal to 94% with validation accuracy as 89%. For other combinations, training accuracy was 91% and below with validation accuracy 88% and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "54-zdrzyRTEa",
    "outputId": "b511708c-450c-4c58-82cd-6dec80b33ac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 0 ... 3 2 2]\n",
      "CPU times: user 53.1 s, sys: 10 ms, total: 53.2 s\n",
      "Wall time: 53.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test_svm=svm.predict(X_test)\n",
    "print(y_test_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UKrmDLE8eATa"
   },
   "source": [
    "SVM was chosen as the most optimal model and was used to predict the results of test model. \n",
    "\n",
    "The runtime of calculating test results is 53.2 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "qT0GlfGoQujZ",
    "outputId": "fa6188bf-7a6c-4c11-a434-413acbf8a545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id  Label\n",
      "0        0      3\n",
      "1        1      4\n",
      "2        2      0\n",
      "3        3      1\n",
      "4        4      1\n",
      "...    ...    ...\n",
      "9995  9995      1\n",
      "9996  9996      1\n",
      "9997  9997      3\n",
      "9998  9998      2\n",
      "9999  9999      2\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame((y_test_svm),columns = ['Label'])\n",
    "Id = pd.read_csv('testX.csv')[['Id']]\n",
    "submission = pd.concat([Id,result], axis=1)\n",
    "submission.columns = ['Id', 'Label']\n",
    "submission.to_csv('submission(svm).csv',index=False)\n",
    "\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iEAkf8h3ef2R"
   },
   "source": [
    "ROC is used to check the sensitivity of the datapoints against the decision boundry. The closer they are to the top left corner, the better the results are as they correspond to more area under the curve. The greater the area captured by the curve, the better as the area represents the random points for which the results would be true positives and the are outside the curve are the ones for which the results would be false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "rC56JdTrB2oL",
    "outputId": "47fb41c1-74b3-44a0-a20c-59b205b5f253"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for ROC curve \n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = y_test_svm\n",
    "y_pred = label_binarize(y_pred,classes=[0,1,2,3,4])\n",
    "\n",
    "y_test = label_binarize(y_val,classes=[0,1,2,3,4])\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(5):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[0], tpr[0],lw=lw, label='ROC curve class 0 (area = %0.2f)' % roc_auc[0])\n",
    "plt.plot(fpr[1], tpr[1],lw=lw, label='ROC curve class 1 (area = %0.2f)' % roc_auc[1])\n",
    "plt.plot(fpr[2], tpr[2],lw=lw, label='ROC curve class 2 (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot(fpr[3], tpr[3],lw=lw, label='ROC curve class 3 (area = %0.2f)' % roc_auc[3])\n",
    "plt.plot(fpr[4], tpr[4],lw=lw, label='ROC curve class 4 (area = %0.2f)' % roc_auc[4])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC SVM')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y83Q6Hsngkjn"
   },
   "source": [
    "Class 0 captures maximum area followed by class 4, class 1, class 3 and minimum by class 2. From this we can say that class 0 will have more true positive than any other classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EL-Z6s0mD8lB"
   },
   "source": [
    "\n",
    "Questions stated in the assignment:\n",
    "1. Explanation of Design and implementation of choices of your models - all the models that were considered during the execution of the model have been explained along with the code snippets.\n",
    "2. Implementation of design choices - All the choices that were made have there codes snippets along with explanation.\n",
    "3. Kaggle Competition Score - 89.180%\n",
    "4. Results Analysis:\n",
    "        a. Runtime performance: \n",
    "        Training - 580 seconds, validation prediction - 37 seconds, test prediction - 41.3 seconds.\n",
    "        b. Comparision of different algorithms and parameters: \n",
    "        For algorithm, all the combinations of parameters that were tried have been stated. The results between different algorithms was distinguished based on test accuracy and time taken to produce results.\n",
    "    For preprocessing - Standard Scaler was chosen over MinMax: The major difference between the two is that the zscore centers the data by subtracting the mean and dividing by standard deviation(sets the overall mean as zero and standard deviation as one) whereas, minmax scaler rescales the data by setting minimum to 0 and maximum to 1(by default) and all the other values scale accordingly. The end goal is to maximize the variance such that dimensionality reduction could work better. For this, if we go by values itself, we would be able to distinguish by the fact that minmax ranges the data between 0 and 1 whereas standardscaler has ~68% data ranging between -1 and 1 and ~95% between-1.96 and 1.96, which shows that data is more widespread in standardscaler than in minmax.\n",
    "    Also, when tested on SVM, the accuarcy from minmax was 20% for validation set and 89% from StandardScaler\n",
    "    For dimensionality reduction: out of two major sub selections - PCA and t-SNE, PCA was chosen based on the plots generated and time taken to compile results. t-SNE should be rejected as it took ~8 hours to compute the model whereas PCA took ~321 seconds to generate the results(when n_componenets were not specified). Hence PCA was a better option than any other method.\n",
    "    For classification: out of KNN, XGBoost and SVM, SVM was chosen as the SVM generated maximum model accuracya nd took least time out of the three models to generate results. The hyperparameter tuning of each of the algorithms have been discussed above in detail. The time taken and the accuarcy generated have been staed alongside.\n",
    "        c. Explanation of your model: \n",
    "    Support Vector Machine tries to find a hyperplane that correctly classifies the data into one of the defined groups. The goal of SVM is to maximize the distance between the hyperplanes making sure that it does not misclassifiy the data. SVM has been able to outperform all the other classification models tested so far. The maximum accuracy reported on the test data is 89.2%(reported for 50% of the data). As defined above, the following parameters were hypertuned. The kernel was set to 'rbf' as linear took a lot of time to compile the results and the difference in the accuarcy of test result was also marginal. For the other parameters, the model performed the best at c=10 and gamma=0.001.\n",
    "\n",
    "    The factors that determine which c value is the best are:\n",
    "\n",
    "    to have hyperplanes that do not misclassify the data\n",
    "    to have the largest minimum margin between hyperplanes\n",
    "    C at 10 and 100 had minimal difference when the results were compared. Rather a slight dip in the accuracy for c=100 was observed. also considering the above statements, we can say that '10' best satisfies the purpose of the model as it would have similar misclassification as 100 and will have larger margin between the hyperplanes as compared to 100.\n",
    "\n",
    "    Also, having lower curvature gave better results than higher gamma value which suggests that data is not wide spread, hence having lower curvature better performs on the results.\n",
    "\n",
    "    The time recorded for calculating training accuracy and test accuarcy was better than XGBoost, therefore, we can say that SVM outperforms all the other models and their constraints that were tested on the dataset.\n",
    "\n",
    "    The runtime for fitting the model and calculating the accuracy of training model was ~580 seconds.\n",
    "\n",
    "    The accuracy obtained for training was maximum for c=10 and gamma =0.01 and was equal to 94% with validation accuracy as 89%. For other combinations, training accuracy was 91% and below with validation accuracy 88% and below.\n",
    "        d. ROC Curve: \n",
    "        The plot along with its explanation has been stated above.\n",
    "        e. Evaluate your code: \n",
    "        The code has been split into 90-10 ratio for comparision. The training data has been tested on various hyperparameters and best results ahve been calculated.\n",
    "    The accuracy obtained for training was maximum for c=10 and gamma =0.01 and was equal to 94% with validation accuracy as 89%. For other combinations, training accuracy was 91% and below with validation accuracy 88% and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 3 a).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
