{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FhTgqB5KudM"
   },
   "source": [
    "# Assignment 3\n",
    "### Question 2 - CNN model\n",
    "\n",
    "In this part of the assignment, the same classification problem will be resolved using CNN architecture. The modelling started with a simple layer and later more layers were added to improvise the accuracy. The code below walks you through all the steps that led to the final layer structuring.\n",
    "\n",
    "The dataset is fashion MNIST with 786 columns and 60,000 observations. Due to the large size of data and incompatible system configuration, the implementation was moved to google colabs. This dataset is the representation of five different kinds of fashion items. Each row represents one fashion item and every column of the row stores the pixel value used to generate the image of the fashion item. Based on these values, we have to classify the data into 5 distinct categories.\n",
    " \n",
    "The different methods used for implementation and the reasons why, have been discussed along with the code snippets, combining question 1 and 2 asked in the assignment.\n",
    "\n",
    "Also, some sections had to interrupt when executing the second time due to long hours they were taking for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sNMQR5JSfw_8"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import keras\n",
    "import keras.utils\n",
    "from keras.models import Sequential\n",
    "from keras import utils as np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization, AveragePooling2D\n",
    "from keras import backend as k\n",
    "import time as time\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.constraints import max_norm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0e2pF-1GOMc_"
   },
   "source": [
    "The implementation started with loading the train and test dataset. The test dataset had 10,000 observations on which the accuracy of the model was calculated. The train dataset was then split into train and validation set(10% of the total data), to check the validity of the model before running it on the test data. The data was transformed into a 28X28 matrix to generate the images for better visual understanding using imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "1TCGq-wokv_Y",
    "outputId": "6fc38d74-d28c-4bd7-98f9-98cbb8dc2831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Features: (54000, 784)\n",
      "Shape of Validate Features: (6000, 784)\n",
      "Shape of Training labels: (54000,)\n",
      "Shape of Validation labels: (6000,)\n",
      "Shape of Testing Features: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "data_train_orig = pd.read_csv('train.csv')\n",
    "\n",
    "y_train_0 = data_train_orig['Label']\n",
    "\n",
    "X_train_0 = data_train_orig.drop(\"Id\", axis = 1)   # Dropping the Id column as it seems to be redundant\n",
    "X_train_1 = X_train_0.drop(\"Label\", axis = 1)\n",
    "\n",
    "seed = 42\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_1, y_train_0, random_state = seed, test_size = 0.1)                                       # Dropping the column of target labels to retain only the feature values\n",
    "\n",
    "data_test_orig = pd.read_csv('testX.csv')\n",
    "\n",
    "X_test = data_test_orig.drop(\"Id\", axis = 1)   \n",
    "\n",
    "print(\"Shape of Training Features:\", X_train.shape)\n",
    "print(\"Shape of Validate Features:\", X_val.shape)\n",
    "print(\"Shape of Training labels:\", y_train.shape)\n",
    "print(\"Shape of Validation labels:\", y_val.shape)\n",
    "print(\"Shape of Testing Features:\", X_test.shape)\n",
    "# print(\"Shape of Training Features:\", y_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bhk6YPwoQBr6"
   },
   "source": [
    "The total number of unique classes were checked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "a5VA82penpEx",
    "outputId": "5aa80cd7-3eda-4dda-9e6d-602b6a20fdd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of outputs :  5\n",
      "Output classes :  [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(y_train)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "emhvUb0bgHPk"
   },
   "source": [
    "They are reshaped in the shape the model expects them to be in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKRVboYen2Tr"
   },
   "outputs": [],
   "source": [
    "# Image Data format\n",
    "# Grayscale images are loaded as a 2-D array. Before they can be used for modeling, an explicit channel dimension to the image in necessary. \n",
    "# This does not add new data; instead, it changes the array data structure to have an additional third axis with one dimension to hold the grayscale pixel values.\n",
    "\n",
    "img_rows, img_cols = 28,28\n",
    "\n",
    "# Keras have default as channel last\n",
    "X_train = X_train.values.reshape(X_train.shape[0], img_rows, img_cols, 1)  \n",
    "X_val = X_val.values.reshape(X_val.shape[0], img_rows, img_cols, 1) \n",
    "X_test = X_test.values.reshape(X_test.shape[0], img_rows, img_cols, 1)         \n",
    "X_inp = (img_rows, img_cols, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NYaOdrOtoFUp",
    "outputId": "52796d19-d078-446d-86c5-2706b3893024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :  (6000, 28, 28, 1) (54000,)\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape : ', X_val.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9H-whky_QQjb"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f9oqFM4MVD7f"
   },
   "source": [
    "Since the image pixel values ranges between 0 and 255, the data is normalized by dividing each pixel by the maximum of the range i.e. 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgTKPM_6oLr2"
   },
   "outputs": [],
   "source": [
    "# Normalizing data\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeKj0UvgVZk9"
   },
   "source": [
    "By doing this step, we create 5 columns corresponding to each of the 5 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Us-RYHMVoPUe"
   },
   "outputs": [],
   "source": [
    "# 5 Target classes: 0 - 4\n",
    "# keras.utils.to_categorical will create 5 columns\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train) \n",
    "y_val = keras.utils.to_categorical(y_val)\n",
    "#y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jL6BbyXrXNe8"
   },
   "source": [
    "The following variables were set after testing them with different combinations. The batch_size was tested on smaller sizes but that resulted in the model taking a lot of time to compute the results. Therefore, 512 was chosen as the final size.\n",
    "\n",
    "Epochs were also tested on 20, 50, 100 and 200. It was observed that the increase in accuracy between 100 and 200 was very slow compared to that of 0-100. Therefore, the number of epochs were set as 200 because they did not increase beyond that point. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pA5cWf9KoTXX"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 200\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eokbThcXbQ7f"
   },
   "source": [
    "Convolution is nothing but weighted sum of pixel values of the image. As we define the size of the sliding window, the convolution calculates the weight over those values under consideration. As the window slides, it convolves the window with the images to generate a new value for the image pixel. Once the values are generated, max pooling is executed. Max pooling is used for reducing the dimensionality of the image. What is does is it picks up the maximum values from the set of values within the window and allocates it as the new value of the reduced dimension. This window keeps on sliding till a new layer with reduced dimensionality is produced. But the layers produced are 3D in nature and for a fully connected network, we need a vector of values. To do that flatten() is used which converts it into a vector. Lastly, Dense layer is a fully connected network just like an ANN. Dropout layers are also added to drop the neurons in the layer i.e. if p=0.5, then 50% of the neurons are dropped. Batch Normalization can also be done. It is another way to regularizer the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5sOx6VwtVse"
   },
   "source": [
    "# Hyperparameter tuning \n",
    "Hyerparameter tuning is another part that need to be considered while convolving the layers. These are usually generated by testing and trying different values. For example, what kindly of padding shall be done(same is preserved as it borders the outline with same values as the previous layer), what kind of activation function shall be chosen(linear or relu). I tried with linear and later adding a layer of LeakyRelu layer but that hampered the performance of the model, therefore activation function was directly set to relu. Different combinations with dropout were tested as well but setting it equal to 0.5 yielded best results.\n",
    "\n",
    "Also, the activation function is set to relu as it resolves the problem of vanishing gradient. Thats why, when the activation was set to linear, the accuracy was dropped because of vanishing gradient.\n",
    "\n",
    "kernel size is set to 3X3 for Conv2D as this the smallest window on which the model performs best convolution. Even the number of filters are chosen to be small so that the model does not overfit the training set. The initial value is small but as the parameters increase, the number of filters change accordingly.\n",
    "\n",
    "For max_pooling, the window size is 2X2 so that no information is lost as well as it is made sure that the edges are sharpened. For Dense, a fully connected network is designed, therfore the transition from given nodes to the output nodes shall be smooth and it shall be made sure that no information is lost on the way. Adding too many layers can also lead to overfitting the train data resulting in lower accuracy.\n",
    "\n",
    "Usually the CNN initial layers work as edge detector and as we increase the number of layers more features start becoming prominent. Max pooling after CNN makes edges noticeable. Also dense layers help in retaining the properties of the data, therefore more the layers, more hidden features can be fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6B0c4Os2Zh79"
   },
   "outputs": [],
   "source": [
    "# Model 1\n",
    "# fashion_model = Sequential()\n",
    "# fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(28,28,1)))\n",
    "# fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "# fashion_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "# fashion_model.add(Flatten())\n",
    "# fashion_model.add(Dense(128, activation='linear'))\n",
    "# fashion_model.add(LeakyReLU(alpha=0.1))           \n",
    "# fashion_model.add(Dropout(0.5))\n",
    "# fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 72% accuracy on test data, training - 85%, val - 76%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w46WjYu5wfBY"
   },
   "source": [
    "I tried with linear and later adding a layer of LeakyRelu layer but that hampered the performance of the model, therefore activation function was directly set to relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TWCmHteucVj"
   },
   "outputs": [],
   "source": [
    "# Model 2\n",
    "# fashion_model = Sequential()\n",
    "# fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding='same',input_shape=(28,28,1)))\n",
    "# fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "# fashion_model.add(Dropout(0.5))\n",
    "# fashion_model.add(Flatten())\n",
    "# fashion_model.add(Dense(128, activation='relu'))\n",
    "# fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 82.4% accuracy on test data, training - 89%, val - 84%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Udpm_vNww1kX"
   },
   "source": [
    "Increasing the number of convolution layer improvised the accuracy on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCRAR3ADoZsb"
   },
   "outputs": [],
   "source": [
    "# Model 3\n",
    "# fashion_model = Sequential()\n",
    "# fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding='same',input_shape=(28,28,1)))\n",
    "# fashion_model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
    "# fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "# fashion_model.add(Dropout(0.5))\n",
    "# fashion_model.add(Flatten())\n",
    "# fashion_model.add(Dense(128, activation='relu'))\n",
    "# fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 84.6% accuracy on test data, training - 92%, val - 86%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ADOb14HGxMb9"
   },
   "source": [
    "Adding another segment of convolution layer and max pooling with dropout increased the performance but time taken was also increased, therefore from the second segment one convolution layer was removed, leaving only one in that segment. It was observed that the change in accuarcy was marginal but the time was improvised be a great extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WnSIzMOlxAmo"
   },
   "outputs": [],
   "source": [
    "# Model 4\n",
    "# fashion_model = Sequential()\n",
    "# fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding='same',input_shape=(28,28,1)))\n",
    "# fashion_model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
    "# fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "# fashion_model.add(Dropout(0.5))\n",
    "# fashion_model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))           \n",
    "# fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "# fashion_model.add(Dropout(0.5))\n",
    "# fashion_model.add(Flatten())\n",
    "# fashion_model.add(Dense(128, activation='relu'))\n",
    "# fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "#86.7% accuracy on test data, training - 95%, val - 87%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ToU7B4AIyEEk"
   },
   "source": [
    "For regularizing the data, Batch normalization was added. But it was observed that this led to decrease in the validation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5rFkUuuyHaQ"
   },
   "outputs": [],
   "source": [
    "# Model 5\n",
    "# fashion_model = Sequential()\n",
    "# fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding='same',input_shape=(28,28,1)))\n",
    "# fashion_model.add(BatchNormalization())\n",
    "# fashion_model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
    "# fashion_model.add(BatchNormalization())\n",
    "# fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "# fashion_model.add(Dropout(0.5))\n",
    "# fashion_model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))  \n",
    "# fashion_model.add(BatchNormalization())          \n",
    "# fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "# fashion_model.add(Dropout(0.5))\n",
    "# fashion_model.add(Flatten())\n",
    "# fashion_model.add(Dense(128, activation='relu'))\n",
    "# fashion_model.add(BatchNormalization())\n",
    "# fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "#80.3% accuracy on test data, training - 89%, val - 82%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFE_f6YByuDP"
   },
   "source": [
    "To improvise on the model, Batch Normalization was removed from the dense layers as it regularized the data(features) that were extracted from the fully connected layer. This led to loss of information, hence it was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R0irpkmtykic"
   },
   "outputs": [],
   "source": [
    "# model 6\n",
    "# fashion_model = Sequential()\n",
    "# fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding='same',input_shape=(28,28,1)))\n",
    "# fashion_model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
    "# fashion_model.add(BatchNormalization())\n",
    "# fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "# fashion_model.add(Dropout(0.5))\n",
    "# fashion_model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))  \n",
    "# fashion_model.add(BatchNormalization())          \n",
    "# fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "# fashion_model.add(Dropout(0.5))\n",
    "# fashion_model.add(Flatten())\n",
    "# fashion_model.add(Dense(128, activation='relu'))\n",
    "# fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "#88% accuracy on test data, training - 97%, val - 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Aw7Drzrsbok"
   },
   "outputs": [],
   "source": [
    "# model 7\n",
    "# import keras\n",
    "# from keras.models import Sequential,Input,Model\n",
    "# from keras.layers import Dense, Dropout, Flatten\n",
    "# from keras.layers import Conv2D, MaxPooling2D\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# fashion_model = Sequential()\n",
    "# fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding='same',input_shape=(28,28,1)))\n",
    "# fashion_model.add(BatchNormalization())\n",
    "# fashion_model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
    "# fashion_model.add(BatchNormalization())\n",
    "# fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "# fashion_model.add(Dropout(0.5))\n",
    "# fashion_model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))  \n",
    "# fashion_model.add(BatchNormalization())          \n",
    "# fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "# fashion_model.add(Dropout(0.5))\n",
    "# fashion_model.add(Flatten())\n",
    "# fashion_model.add(Dense(128, activation='relu'))\n",
    "# fashion_model.add(Dense(64, activation='relu'))\n",
    "# fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "# 90.7% accuracy on test data, training - 97%, val - 91%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7U0tfwwzJUS"
   },
   "source": [
    "Finally, adding another Dense layer led to the generation of best accuracy of the model. The reported accuracy of the model was 90.72% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eF70kJGM2-5U"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding='same',input_shape=(28,28,1)))\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Dropout(0.5))\n",
    "fashion_model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))  \n",
    "fashion_model.add(BatchNormalization())          \n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Dropout(0.5))\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='relu'))\n",
    "fashion_model.add(Dense(64, activation='relu'))\n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FYfnKqrM0EPX"
   },
   "source": [
    "This was another architecture that was tested but the results were not upto satisfaction. Adding additional Conv2d layer results in overfitting because of which the accuracy is affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXCTvsSsWyeh"
   },
   "outputs": [],
   "source": [
    "# fashion_model = Sequential()\n",
    "# fashion_model.add(Conv2D(32, (3, 3),activation='relu', padding=\"same\",input_shape=(28,28,1)))\n",
    "# fashion_model.add(BatchNormalization())\n",
    "# fashion_model.add(Conv2D(32, (3, 3),activation='relu', padding=\"same\"))\n",
    "# fashion_model.add(BatchNormalization())\n",
    "# fashion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# fashion_model.add(Dropout(0.5))\n",
    "# # second CONV => RELU => CONV => RELU => POOL layer set\n",
    "# fashion_model.add(Conv2D(64, (3, 3),activation='relu', padding=\"same\"))\n",
    "# fashion_model.add(BatchNormalization(axis=-1))\n",
    "# fashion_model.add(Conv2D(64, (3, 3),activation='relu', padding=\"same\"))\n",
    "# fashion_model.add(BatchNormalization(axis=-1))\n",
    "# fashion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# fashion_model.add(Dropout(0.5))\n",
    "# # first (and only) set of FC => RELU layers\n",
    "# fashion_model.add(Flatten())\n",
    "# fashion_model.add(Dense(512,activation='relu'))\n",
    "# fashion_model.add(BatchNormalization())\n",
    "# fashion_model.add(Dropout(0.5))\n",
    "# # softmax classifier\n",
    "# fashion_model.add(Dense(num_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UrkDAdWD0GcN"
   },
   "source": [
    "When it came to optimizers, many optimizers were tested such as SGD, Adagrad, AdaDelta, RMSProp and Adam. Out of these three, only Adam was able to generate best results because it is an improvisation of all the optimizers it captures momentum as well as properties of RMSProp.\n",
    "\n",
    "I also designed an improvisation of Adam as a part of Optimization project but due to shortage of time was unable to test on this dataset. I will add the snapshot of the same for reference sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-EMgOXEoeQC"
   },
   "outputs": [],
   "source": [
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "-A-0E6d5ofmo",
    "outputId": "3802022e-2bb3-4e54-e22e-ef5dc89b112d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 904,965\n",
      "Trainable params: 904,581\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fashion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NkMJsICJoi7Q",
    "outputId": "0e971efd-bbbc-4c65-a060-10c3c084d92b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/200\n",
      "54000/54000 [==============================] - 6s 119us/step - loss: 0.9352 - accuracy: 0.6717 - val_loss: 15.5987 - val_accuracy: 0.1983\n",
      "Epoch 2/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.4785 - accuracy: 0.8124 - val_loss: 16.3492 - val_accuracy: 0.1985\n",
      "Epoch 3/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.4079 - accuracy: 0.8392 - val_loss: 8.4500 - val_accuracy: 0.2745\n",
      "Epoch 4/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.3806 - accuracy: 0.8486 - val_loss: 2.5538 - val_accuracy: 0.3713\n",
      "Epoch 5/200\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.3623 - accuracy: 0.8531 - val_loss: 0.7112 - val_accuracy: 0.7448\n",
      "Epoch 6/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.3287 - accuracy: 0.8661 - val_loss: 0.4278 - val_accuracy: 0.8320\n",
      "Epoch 7/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.3241 - accuracy: 0.8700 - val_loss: 0.3847 - val_accuracy: 0.8475\n",
      "Epoch 8/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.3007 - accuracy: 0.8759 - val_loss: 1.5178 - val_accuracy: 0.6165\n",
      "Epoch 9/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.3008 - accuracy: 0.8772 - val_loss: 0.6621 - val_accuracy: 0.7652\n",
      "Epoch 10/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.2876 - accuracy: 0.8820 - val_loss: 0.2932 - val_accuracy: 0.8825\n",
      "Epoch 11/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.2916 - accuracy: 0.8798 - val_loss: 0.2802 - val_accuracy: 0.8917\n",
      "Epoch 12/200\n",
      "54000/54000 [==============================] - 6s 108us/step - loss: 0.2766 - accuracy: 0.8867 - val_loss: 0.3846 - val_accuracy: 0.8495\n",
      "Epoch 13/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.2715 - accuracy: 0.8894 - val_loss: 0.6873 - val_accuracy: 0.7658\n",
      "Epoch 14/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.2712 - accuracy: 0.8882 - val_loss: 0.2546 - val_accuracy: 0.9022\n",
      "Epoch 15/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.2622 - accuracy: 0.8927 - val_loss: 0.4775 - val_accuracy: 0.8210\n",
      "Epoch 16/200\n",
      "54000/54000 [==============================] - 6s 108us/step - loss: 0.2575 - accuracy: 0.8946 - val_loss: 0.3063 - val_accuracy: 0.8763\n",
      "Epoch 17/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.2537 - accuracy: 0.8965 - val_loss: 0.3425 - val_accuracy: 0.8632\n",
      "Epoch 18/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.2486 - accuracy: 0.8979 - val_loss: 0.3099 - val_accuracy: 0.8773\n",
      "Epoch 19/200\n",
      "54000/54000 [==============================] - 6s 108us/step - loss: 0.2538 - accuracy: 0.8964 - val_loss: 0.2608 - val_accuracy: 0.8925\n",
      "Epoch 20/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.2429 - accuracy: 0.8985 - val_loss: 0.3523 - val_accuracy: 0.8618\n",
      "Epoch 21/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.2424 - accuracy: 0.9011 - val_loss: 0.5622 - val_accuracy: 0.8058\n",
      "Epoch 22/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.2314 - accuracy: 0.9049 - val_loss: 0.2561 - val_accuracy: 0.8983\n",
      "Epoch 23/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.2329 - accuracy: 0.9047 - val_loss: 0.3944 - val_accuracy: 0.8520\n",
      "Epoch 24/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.2373 - accuracy: 0.9025 - val_loss: 0.3335 - val_accuracy: 0.8670\n",
      "Epoch 25/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.2218 - accuracy: 0.9090 - val_loss: 0.3901 - val_accuracy: 0.8522\n",
      "Epoch 26/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.2217 - accuracy: 0.9088 - val_loss: 0.2689 - val_accuracy: 0.8970\n",
      "Epoch 27/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.2203 - accuracy: 0.9092 - val_loss: 0.2549 - val_accuracy: 0.9003\n",
      "Epoch 28/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.2082 - accuracy: 0.9141 - val_loss: 0.2761 - val_accuracy: 0.8910\n",
      "Epoch 29/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.2097 - accuracy: 0.9130 - val_loss: 0.3816 - val_accuracy: 0.8632\n",
      "Epoch 30/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.2022 - accuracy: 0.9175 - val_loss: 0.6739 - val_accuracy: 0.7930\n",
      "Epoch 31/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.2088 - accuracy: 0.9146 - val_loss: 0.3783 - val_accuracy: 0.8603\n",
      "Epoch 32/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.2016 - accuracy: 0.9196 - val_loss: 0.2760 - val_accuracy: 0.9007\n",
      "Epoch 33/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1987 - accuracy: 0.9191 - val_loss: 0.2746 - val_accuracy: 0.9002\n",
      "Epoch 34/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.2031 - accuracy: 0.9164 - val_loss: 0.7325 - val_accuracy: 0.7857\n",
      "Epoch 35/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.2038 - accuracy: 0.9159 - val_loss: 0.2670 - val_accuracy: 0.8988\n",
      "Epoch 36/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1986 - accuracy: 0.9186 - val_loss: 0.2924 - val_accuracy: 0.8887\n",
      "Epoch 37/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1886 - accuracy: 0.9228 - val_loss: 0.2258 - val_accuracy: 0.9143\n",
      "Epoch 38/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1875 - accuracy: 0.9247 - val_loss: 0.2605 - val_accuracy: 0.9033\n",
      "Epoch 39/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1868 - accuracy: 0.9242 - val_loss: 0.2956 - val_accuracy: 0.8857\n",
      "Epoch 40/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1867 - accuracy: 0.9246 - val_loss: 0.2381 - val_accuracy: 0.9100\n",
      "Epoch 41/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1843 - accuracy: 0.9249 - val_loss: 0.3721 - val_accuracy: 0.8672\n",
      "Epoch 42/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1773 - accuracy: 0.9283 - val_loss: 0.3178 - val_accuracy: 0.8847\n",
      "Epoch 43/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1786 - accuracy: 0.9272 - val_loss: 0.2831 - val_accuracy: 0.8923\n",
      "Epoch 44/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1740 - accuracy: 0.9291 - val_loss: 0.2867 - val_accuracy: 0.8963\n",
      "Epoch 45/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1684 - accuracy: 0.9320 - val_loss: 0.2935 - val_accuracy: 0.8905\n",
      "Epoch 46/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1670 - accuracy: 0.9313 - val_loss: 0.2368 - val_accuracy: 0.9188\n",
      "Epoch 47/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1741 - accuracy: 0.9286 - val_loss: 0.2988 - val_accuracy: 0.8918\n",
      "Epoch 48/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1773 - accuracy: 0.9289 - val_loss: 0.3766 - val_accuracy: 0.8665\n",
      "Epoch 49/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1695 - accuracy: 0.9319 - val_loss: 0.2710 - val_accuracy: 0.9023\n",
      "Epoch 50/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1632 - accuracy: 0.9341 - val_loss: 0.2575 - val_accuracy: 0.9043\n",
      "Epoch 51/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1654 - accuracy: 0.9322 - val_loss: 0.2362 - val_accuracy: 0.9162\n",
      "Epoch 52/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1608 - accuracy: 0.9340 - val_loss: 0.2628 - val_accuracy: 0.9057\n",
      "Epoch 53/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1606 - accuracy: 0.9339 - val_loss: 0.3254 - val_accuracy: 0.8838\n",
      "Epoch 54/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1587 - accuracy: 0.9360 - val_loss: 0.3301 - val_accuracy: 0.8820\n",
      "Epoch 55/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1596 - accuracy: 0.9365 - val_loss: 0.2905 - val_accuracy: 0.8987\n",
      "Epoch 56/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1543 - accuracy: 0.9366 - val_loss: 0.2699 - val_accuracy: 0.9045\n",
      "Epoch 57/200\n",
      "54000/54000 [==============================] - 6s 108us/step - loss: 0.1544 - accuracy: 0.9376 - val_loss: 0.2458 - val_accuracy: 0.9115\n",
      "Epoch 58/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1528 - accuracy: 0.9382 - val_loss: 0.4082 - val_accuracy: 0.8638\n",
      "Epoch 59/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1528 - accuracy: 0.9380 - val_loss: 0.2551 - val_accuracy: 0.9085\n",
      "Epoch 60/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1587 - accuracy: 0.9361 - val_loss: 0.2586 - val_accuracy: 0.9095\n",
      "Epoch 61/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1470 - accuracy: 0.9403 - val_loss: 0.3061 - val_accuracy: 0.8938\n",
      "Epoch 62/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1452 - accuracy: 0.9410 - val_loss: 0.2672 - val_accuracy: 0.9043\n",
      "Epoch 63/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1454 - accuracy: 0.9409 - val_loss: 0.2626 - val_accuracy: 0.9108\n",
      "Epoch 64/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1447 - accuracy: 0.9417 - val_loss: 0.3179 - val_accuracy: 0.8897\n",
      "Epoch 65/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1470 - accuracy: 0.9421 - val_loss: 0.2597 - val_accuracy: 0.9063\n",
      "Epoch 66/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1396 - accuracy: 0.9442 - val_loss: 0.2690 - val_accuracy: 0.9052\n",
      "Epoch 67/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1418 - accuracy: 0.9432 - val_loss: 0.2623 - val_accuracy: 0.9100\n",
      "Epoch 68/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1392 - accuracy: 0.9447 - val_loss: 0.2491 - val_accuracy: 0.9133\n",
      "Epoch 69/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1356 - accuracy: 0.9463 - val_loss: 0.2497 - val_accuracy: 0.9140\n",
      "Epoch 70/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1345 - accuracy: 0.9464 - val_loss: 0.2643 - val_accuracy: 0.9137\n",
      "Epoch 71/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1319 - accuracy: 0.9470 - val_loss: 0.2711 - val_accuracy: 0.9062\n",
      "Epoch 72/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1315 - accuracy: 0.9478 - val_loss: 0.3149 - val_accuracy: 0.8948\n",
      "Epoch 73/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1318 - accuracy: 0.9469 - val_loss: 0.2765 - val_accuracy: 0.9125\n",
      "Epoch 74/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1375 - accuracy: 0.9451 - val_loss: 0.2997 - val_accuracy: 0.9023\n",
      "Epoch 75/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1293 - accuracy: 0.9487 - val_loss: 0.2698 - val_accuracy: 0.9107\n",
      "Epoch 76/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1388 - accuracy: 0.9441 - val_loss: 0.2770 - val_accuracy: 0.9080\n",
      "Epoch 77/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1283 - accuracy: 0.9487 - val_loss: 0.2530 - val_accuracy: 0.9157\n",
      "Epoch 78/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1396 - accuracy: 0.9432 - val_loss: 0.4899 - val_accuracy: 0.8538\n",
      "Epoch 79/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1272 - accuracy: 0.9496 - val_loss: 0.2728 - val_accuracy: 0.9092\n",
      "Epoch 80/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1254 - accuracy: 0.9510 - val_loss: 0.2783 - val_accuracy: 0.9107\n",
      "Epoch 81/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1285 - accuracy: 0.9488 - val_loss: 0.3180 - val_accuracy: 0.9030\n",
      "Epoch 82/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1231 - accuracy: 0.9507 - val_loss: 0.2769 - val_accuracy: 0.9122\n",
      "Epoch 83/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1165 - accuracy: 0.9533 - val_loss: 0.2608 - val_accuracy: 0.9158\n",
      "Epoch 84/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1221 - accuracy: 0.9514 - val_loss: 0.2710 - val_accuracy: 0.9152\n",
      "Epoch 85/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1232 - accuracy: 0.9508 - val_loss: 0.4080 - val_accuracy: 0.8728\n",
      "Epoch 86/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1262 - accuracy: 0.9496 - val_loss: 0.3059 - val_accuracy: 0.9033\n",
      "Epoch 87/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1228 - accuracy: 0.9502 - val_loss: 0.2823 - val_accuracy: 0.9120\n",
      "Epoch 88/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1198 - accuracy: 0.9525 - val_loss: 0.4675 - val_accuracy: 0.8660\n",
      "Epoch 89/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1201 - accuracy: 0.9503 - val_loss: 0.2923 - val_accuracy: 0.9090\n",
      "Epoch 90/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1165 - accuracy: 0.9536 - val_loss: 0.3025 - val_accuracy: 0.9060\n",
      "Epoch 91/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.1186 - accuracy: 0.9526 - val_loss: 0.2687 - val_accuracy: 0.9158\n",
      "Epoch 92/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1148 - accuracy: 0.9552 - val_loss: 0.3795 - val_accuracy: 0.8830\n",
      "Epoch 93/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1149 - accuracy: 0.9551 - val_loss: 0.3430 - val_accuracy: 0.8957\n",
      "Epoch 94/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1122 - accuracy: 0.9548 - val_loss: 0.4000 - val_accuracy: 0.8808\n",
      "Epoch 95/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1145 - accuracy: 0.9547 - val_loss: 0.3140 - val_accuracy: 0.9058\n",
      "Epoch 96/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1097 - accuracy: 0.9570 - val_loss: 0.3074 - val_accuracy: 0.9065\n",
      "Epoch 97/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1104 - accuracy: 0.9557 - val_loss: 0.2983 - val_accuracy: 0.9102\n",
      "Epoch 98/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1157 - accuracy: 0.9531 - val_loss: 0.3251 - val_accuracy: 0.8978\n",
      "Epoch 99/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1106 - accuracy: 0.9562 - val_loss: 0.2860 - val_accuracy: 0.9123\n",
      "Epoch 100/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1100 - accuracy: 0.9570 - val_loss: 0.3607 - val_accuracy: 0.8937\n",
      "Epoch 101/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1120 - accuracy: 0.9557 - val_loss: 0.3186 - val_accuracy: 0.9027\n",
      "Epoch 102/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1080 - accuracy: 0.9572 - val_loss: 0.4002 - val_accuracy: 0.8808\n",
      "Epoch 103/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1054 - accuracy: 0.9591 - val_loss: 0.2982 - val_accuracy: 0.9027\n",
      "Epoch 104/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1014 - accuracy: 0.9591 - val_loss: 0.3677 - val_accuracy: 0.8892\n",
      "Epoch 105/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1136 - accuracy: 0.9559 - val_loss: 0.4497 - val_accuracy: 0.8693\n",
      "Epoch 106/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1100 - accuracy: 0.9571 - val_loss: 0.2941 - val_accuracy: 0.9132\n",
      "Epoch 107/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.1094 - accuracy: 0.9566 - val_loss: 0.3436 - val_accuracy: 0.8933\n",
      "Epoch 108/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.1053 - accuracy: 0.9589 - val_loss: 0.3071 - val_accuracy: 0.9050\n",
      "Epoch 109/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1020 - accuracy: 0.9602 - val_loss: 0.3524 - val_accuracy: 0.8957\n",
      "Epoch 110/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1074 - accuracy: 0.9571 - val_loss: 0.3191 - val_accuracy: 0.9027\n",
      "Epoch 111/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.0971 - accuracy: 0.9627 - val_loss: 0.6532 - val_accuracy: 0.8452\n",
      "Epoch 112/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.1090 - accuracy: 0.9570 - val_loss: 0.3100 - val_accuracy: 0.9088\n",
      "Epoch 113/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1054 - accuracy: 0.9578 - val_loss: 0.4421 - val_accuracy: 0.8762\n",
      "Epoch 114/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1001 - accuracy: 0.9601 - val_loss: 0.4071 - val_accuracy: 0.8852\n",
      "Epoch 115/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1071 - accuracy: 0.9580 - val_loss: 0.2905 - val_accuracy: 0.9140\n",
      "Epoch 116/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0906 - accuracy: 0.9639 - val_loss: 0.4023 - val_accuracy: 0.8842\n",
      "Epoch 117/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0981 - accuracy: 0.9616 - val_loss: 0.3047 - val_accuracy: 0.9118\n",
      "Epoch 118/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0986 - accuracy: 0.9609 - val_loss: 0.3124 - val_accuracy: 0.9105\n",
      "Epoch 119/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0995 - accuracy: 0.9607 - val_loss: 0.3746 - val_accuracy: 0.8900\n",
      "Epoch 120/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0964 - accuracy: 0.9621 - val_loss: 0.4331 - val_accuracy: 0.8760\n",
      "Epoch 121/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.1008 - accuracy: 0.9611 - val_loss: 0.3207 - val_accuracy: 0.9072\n",
      "Epoch 122/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.1055 - accuracy: 0.9589 - val_loss: 0.3155 - val_accuracy: 0.9037\n",
      "Epoch 123/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0912 - accuracy: 0.9651 - val_loss: 0.3732 - val_accuracy: 0.8912\n",
      "Epoch 124/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0934 - accuracy: 0.9631 - val_loss: 0.4414 - val_accuracy: 0.8783\n",
      "Epoch 125/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0983 - accuracy: 0.9611 - val_loss: 0.3295 - val_accuracy: 0.9012\n",
      "Epoch 126/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0936 - accuracy: 0.9638 - val_loss: 0.4251 - val_accuracy: 0.8775\n",
      "Epoch 127/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0929 - accuracy: 0.9638 - val_loss: 0.4494 - val_accuracy: 0.8737\n",
      "Epoch 128/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0948 - accuracy: 0.9633 - val_loss: 0.3479 - val_accuracy: 0.8993\n",
      "Epoch 129/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0963 - accuracy: 0.9624 - val_loss: 0.3315 - val_accuracy: 0.9060\n",
      "Epoch 130/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0881 - accuracy: 0.9658 - val_loss: 0.3209 - val_accuracy: 0.9113\n",
      "Epoch 131/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0922 - accuracy: 0.9647 - val_loss: 0.3273 - val_accuracy: 0.9063\n",
      "Epoch 132/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.0901 - accuracy: 0.9641 - val_loss: 0.3136 - val_accuracy: 0.9127\n",
      "Epoch 133/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0879 - accuracy: 0.9656 - val_loss: 0.4068 - val_accuracy: 0.8865\n",
      "Epoch 134/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0872 - accuracy: 0.9657 - val_loss: 0.3179 - val_accuracy: 0.9045\n",
      "Epoch 135/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0858 - accuracy: 0.9665 - val_loss: 0.4137 - val_accuracy: 0.8830\n",
      "Epoch 136/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0904 - accuracy: 0.9641 - val_loss: 0.6385 - val_accuracy: 0.8512\n",
      "Epoch 137/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0919 - accuracy: 0.9641 - val_loss: 0.3487 - val_accuracy: 0.9018\n",
      "Epoch 138/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0926 - accuracy: 0.9642 - val_loss: 0.3672 - val_accuracy: 0.8960\n",
      "Epoch 139/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0867 - accuracy: 0.9658 - val_loss: 0.3868 - val_accuracy: 0.8935\n",
      "Epoch 140/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0856 - accuracy: 0.9662 - val_loss: 0.3251 - val_accuracy: 0.9068\n",
      "Epoch 141/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0894 - accuracy: 0.9653 - val_loss: 0.3464 - val_accuracy: 0.9047\n",
      "Epoch 142/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0827 - accuracy: 0.9684 - val_loss: 0.4224 - val_accuracy: 0.8858\n",
      "Epoch 143/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0837 - accuracy: 0.9671 - val_loss: 0.3544 - val_accuracy: 0.9022\n",
      "Epoch 144/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0857 - accuracy: 0.9670 - val_loss: 0.3509 - val_accuracy: 0.9060\n",
      "Epoch 145/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0825 - accuracy: 0.9683 - val_loss: 0.3240 - val_accuracy: 0.9108\n",
      "Epoch 146/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0860 - accuracy: 0.9662 - val_loss: 0.3533 - val_accuracy: 0.8977\n",
      "Epoch 147/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0894 - accuracy: 0.9649 - val_loss: 0.3445 - val_accuracy: 0.9065\n",
      "Epoch 148/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0820 - accuracy: 0.9679 - val_loss: 0.3130 - val_accuracy: 0.9135\n",
      "Epoch 149/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0839 - accuracy: 0.9667 - val_loss: 0.3993 - val_accuracy: 0.8930\n",
      "Epoch 150/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0835 - accuracy: 0.9674 - val_loss: 0.3205 - val_accuracy: 0.9132\n",
      "Epoch 151/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0812 - accuracy: 0.9683 - val_loss: 0.4905 - val_accuracy: 0.8800\n",
      "Epoch 152/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0802 - accuracy: 0.9693 - val_loss: 0.3756 - val_accuracy: 0.8975\n",
      "Epoch 153/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.0803 - accuracy: 0.9681 - val_loss: 0.3481 - val_accuracy: 0.9045\n",
      "Epoch 154/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0857 - accuracy: 0.9668 - val_loss: 0.3336 - val_accuracy: 0.9075\n",
      "Epoch 155/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0814 - accuracy: 0.9688 - val_loss: 0.3810 - val_accuracy: 0.8945\n",
      "Epoch 156/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0783 - accuracy: 0.9697 - val_loss: 0.4347 - val_accuracy: 0.8892\n",
      "Epoch 157/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0855 - accuracy: 0.9670 - val_loss: 0.3354 - val_accuracy: 0.9083\n",
      "Epoch 158/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0770 - accuracy: 0.9703 - val_loss: 0.3370 - val_accuracy: 0.9100\n",
      "Epoch 159/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0797 - accuracy: 0.9691 - val_loss: 0.4375 - val_accuracy: 0.8847\n",
      "Epoch 160/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.0813 - accuracy: 0.9684 - val_loss: 0.9588 - val_accuracy: 0.8172\n",
      "Epoch 161/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0805 - accuracy: 0.9690 - val_loss: 0.3399 - val_accuracy: 0.9145\n",
      "Epoch 162/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0765 - accuracy: 0.9709 - val_loss: 0.5744 - val_accuracy: 0.8590\n",
      "Epoch 163/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.0790 - accuracy: 0.9690 - val_loss: 0.3449 - val_accuracy: 0.9073\n",
      "Epoch 164/200\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.0798 - accuracy: 0.9691 - val_loss: 0.3846 - val_accuracy: 0.8983\n",
      "Epoch 165/200\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.0741 - accuracy: 0.9705 - val_loss: 0.3263 - val_accuracy: 0.9133\n",
      "Epoch 166/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0774 - accuracy: 0.9694 - val_loss: 0.3227 - val_accuracy: 0.9127\n",
      "Epoch 167/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0760 - accuracy: 0.9701 - val_loss: 0.3228 - val_accuracy: 0.9117\n",
      "Epoch 168/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0751 - accuracy: 0.9703 - val_loss: 0.3354 - val_accuracy: 0.9093\n",
      "Epoch 169/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0785 - accuracy: 0.9689 - val_loss: 0.3800 - val_accuracy: 0.9010\n",
      "Epoch 170/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0779 - accuracy: 0.9707 - val_loss: 0.4635 - val_accuracy: 0.8825\n",
      "Epoch 171/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0745 - accuracy: 0.9710 - val_loss: 0.3699 - val_accuracy: 0.9112\n",
      "Epoch 172/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0747 - accuracy: 0.9712 - val_loss: 0.6053 - val_accuracy: 0.8553\n",
      "Epoch 173/200\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.0773 - accuracy: 0.9698 - val_loss: 0.4946 - val_accuracy: 0.8770\n",
      "Epoch 174/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0692 - accuracy: 0.9739 - val_loss: 0.4157 - val_accuracy: 0.8948\n",
      "Epoch 175/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0867 - accuracy: 0.9664 - val_loss: 0.3230 - val_accuracy: 0.9105\n",
      "Epoch 176/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0737 - accuracy: 0.9729 - val_loss: 0.5338 - val_accuracy: 0.8710\n",
      "Epoch 177/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0755 - accuracy: 0.9708 - val_loss: 0.5418 - val_accuracy: 0.8650\n",
      "Epoch 178/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0740 - accuracy: 0.9717 - val_loss: 0.3347 - val_accuracy: 0.9065\n",
      "Epoch 179/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0718 - accuracy: 0.9725 - val_loss: 0.3368 - val_accuracy: 0.9135\n",
      "Epoch 180/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0789 - accuracy: 0.9693 - val_loss: 0.4577 - val_accuracy: 0.8767\n",
      "Epoch 181/200\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.0691 - accuracy: 0.9739 - val_loss: 0.3420 - val_accuracy: 0.9145\n",
      "Epoch 182/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0714 - accuracy: 0.9720 - val_loss: 0.3429 - val_accuracy: 0.9138\n",
      "Epoch 183/200\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.0716 - accuracy: 0.9726 - val_loss: 0.4036 - val_accuracy: 0.8953\n",
      "Epoch 184/200\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.0641 - accuracy: 0.9759 - val_loss: 0.3959 - val_accuracy: 0.8965\n",
      "Epoch 185/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0754 - accuracy: 0.9715 - val_loss: 0.3362 - val_accuracy: 0.9118\n",
      "Epoch 186/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0690 - accuracy: 0.9739 - val_loss: 0.3797 - val_accuracy: 0.8997\n",
      "Epoch 187/200\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.0720 - accuracy: 0.9725 - val_loss: 0.3915 - val_accuracy: 0.9018\n",
      "Epoch 188/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0718 - accuracy: 0.9729 - val_loss: 0.3757 - val_accuracy: 0.9015\n",
      "Epoch 189/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0743 - accuracy: 0.9718 - val_loss: 0.3722 - val_accuracy: 0.8977\n",
      "Epoch 190/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0710 - accuracy: 0.9731 - val_loss: 0.3599 - val_accuracy: 0.9087\n",
      "Epoch 191/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0666 - accuracy: 0.9744 - val_loss: 0.4187 - val_accuracy: 0.8955\n",
      "Epoch 192/200\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.0656 - accuracy: 0.9746 - val_loss: 0.3397 - val_accuracy: 0.9095\n",
      "Epoch 193/200\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.0719 - accuracy: 0.9723 - val_loss: 0.3728 - val_accuracy: 0.9083\n",
      "Epoch 194/200\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.0698 - accuracy: 0.9736 - val_loss: 0.3767 - val_accuracy: 0.9035\n",
      "Epoch 195/200\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.0747 - accuracy: 0.9707 - val_loss: 0.3593 - val_accuracy: 0.9078\n",
      "Epoch 196/200\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.0684 - accuracy: 0.9744 - val_loss: 0.4283 - val_accuracy: 0.8943\n",
      "Epoch 197/200\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.0656 - accuracy: 0.9746 - val_loss: 0.3771 - val_accuracy: 0.9052\n",
      "Epoch 198/200\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.0702 - accuracy: 0.9730 - val_loss: 0.3970 - val_accuracy: 0.8980\n",
      "Epoch 199/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0651 - accuracy: 0.9747 - val_loss: 0.3692 - val_accuracy: 0.9047\n",
      "Epoch 200/200\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 0.0691 - accuracy: 0.9737 - val_loss: 0.3529 - val_accuracy: 0.9148\n"
     ]
    }
   ],
   "source": [
    "history = fashion_model.fit(X_train, y_train, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmgno17E1kZN"
   },
   "source": [
    "The runtime of the model was 1200 seconds in total with each iteration taking 6 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4U21VAPtgUa"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "predict_outcome = fashion_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nBWQ573ft5GC",
    "outputId": "1c494771-86a7-4119-f6f6-ba4dce987811"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_outcome.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hu171VJwsA4h"
   },
   "source": [
    "The runtime for output was 38.9 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XQ_9fHPLuNwr"
   },
   "outputs": [],
   "source": [
    "predict_outcome = np.argmax(np.round(predict_outcome),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vk0Acw-VucOv",
    "outputId": "22d28431-fb45-447f-f38f-e67ea02b35ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_outcome.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "_SgYGTmjusun",
    "outputId": "7750687d-ab77-4b45-e452-b7c24358506f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id  Label\n",
      "0        0      3\n",
      "1        1      4\n",
      "2        2      0\n",
      "3        3      1\n",
      "4        4      0\n",
      "...    ...    ...\n",
      "9995  9995      1\n",
      "9996  9996      1\n",
      "9997  9997      4\n",
      "9998  9998      2\n",
      "9999  9999      2\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame((predict_outcome),columns = ['Label'])\n",
    "Id = pd.read_csv('testX.csv')[['Id']]\n",
    "submission = pd.concat([Id,result], axis=1)\n",
    "submission.columns = ['Id', 'Label']\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "colab_type": "code",
    "id": "jgcxPzX5RAXw",
    "outputId": "32e6dfdd-b7bf-46c7-fc00-47f30c683db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vp9Hn34ystw-"
   },
   "source": [
    "The above plot represents the accurracy and loss incurred by the model. The blue line traces the performance of the train model and the orange line represents the performance of the test set. The first plot shows the change in accuracies as the number of iterations increase whereas the second plot shows the dip in loss as the accuracies change.\n",
    "\n",
    "These plots have only been generated for the final set. \n",
    "\n",
    "The kaggle score is 90.720. The score could have further been improved if other deep learning models were implemented due due to other coursework and submissions, I was able to work on improvising CNN model to its max.\n",
    "\n",
    "Questions asked:\n",
    "\n",
    "1. Explanation of Design and implementation of choices of your models - all the models that were considered during the execution of the model have been explained along with the code snippets.\n",
    "\n",
    "2. Implementation of design choices - All the choices that were made have there codes snippets along with explanation.\n",
    "\n",
    "3. Kaggle Competition Score - 90.720%\n",
    "\n",
    "4. Results Analysis:\n",
    "\n",
    "     a. Runtime performance: \n",
    "     Training - 1200 seconds, test prediction - 38.9 seconds.\n",
    "     b. Comparision of different algorithms and parameters: \n",
    "     Different CNN architectures were compared based on the time taken and accuracies of the train and test sets.\n",
    "     Each of the models have been numbered above. Corresponding to each model, the accuracy has been discussed here.\n",
    "     1. model1(leaky relu)         - train(0.85), val(0.76), test(0.72)  ~4sec per step\n",
    "     2. model2(basic with relu)    - train(0.89), val(0.84), test(0.824) ~5sec per step\n",
    "     3. model3(+CNN Layer)         - train(0.92), val(0.86), test(0.846) ~5sec per step\n",
    "     4. model4(+CNN Layer)         - train(0.95), val(0.87), test(0.867) ~5sec per step\n",
    "     5. model5(+Batch Layer)       - train(0.89), val(0.82), test(0.803) ~11sec per step\n",
    "     6. model6(- extra Batch Layer)- train(0.97), val(0.90), test(0.88)  ~6sec per step\n",
    "     7. model7(final model)        - train(0.97), val(0.91), test(0.907) ~6sec per step\n",
    "     \n",
    "     The hyperparameters in the final model were also altered to conclude this result.\n",
    "     For choosing optimizer : When it came to optimizers, many optimizers were tested such as SGD, Adagrad, AdaDelta, RMSProp and Adam. Out of these three, only Adam was able to generate best results because it is an improvisation of all the optimizers it captures momentum as well as properties of RMSProp.\n",
    "     c. Explanation of my model: Stepwise explanation has been done above. Convolution is nothing but weighted sum of pixel values of the image. As we define the size of the sliding window, the convolution calculates the weight over those values under consideration. As the window slides, it convolves the window with the images to generate a new value for the image pixel. Once the values are generated, max pooling is executed. Max pooling is used for reducing the dimensionality of the image. What is does is it picks up the maximum values from the set of values within the window and allocates it as the new value of the reduced dimension. This window keeps on sliding till a new layer with reduced dimensionality is produced. But the layers produced are 3D in nature and for a fully connected network, we need a vector of values. To do that flatten() is used which converts it into a vector. Lastly, Dense layer is a fully connected network just like an ANN. Dropout layers are also added to drop the neurons in the layer i.e. if p=0.5, then 50% of the neurons are dropped. Batch Normalization can also be done. It is another way to regularizer the data.\n",
    "\n",
    "    Hyerparameter tuning is another part that need to be considered while convolving the layers. These are usually generated by testing and trying different values. For example, what kindly of padding shall be done(same is preserved as it borders the outline with same values as the previous layer), what kind of activation function shall be chosen(linear or relu). I tried with linear and later adding a layer of LeakyRelu layer but that hampered the performance of the model, therefore activation function was directly set to relu. Different combinations with dropout were tested as well but setting it equal to 0.5 yielded best results.\n",
    "\n",
    "    Also, the activation function is set to relu as it resolves the problem of vanishing gradient. Thats why, when the activation was set to linear, the accuracy was dropped because of vanishing gradient.\n",
    "\n",
    "    kernel size is set to 3X3 for Conv2D as this the smallest window on which the model performs best convolution. Even the number of filters are chosen to be small so that the model does not overfit the training set. The initial value is small but as the parameters increase, the number of filters change accordingly.\n",
    "\n",
    "    For max_pooling, the window size is 2X2 so that no information is lost as well as it is made sure that the edges are sharpened. For Dense, a fully connected network is designed, therfore the transition from given nodes to the output nodes shall be smooth and it shall be made sure that no information is lost on the way. Adding too many layers can also lead to overfitting the train data resulting in lower accuracy.\n",
    "\n",
    "    Usually the CNN initial layers work as edge detector and as we increase the number of layers more features start becoming prominent. Max pooling after CNN makes edges noticeable. Also dense layers help in retaining the properties of the data, therefore more the layers, more hidden features can be fetched.\n",
    "     d. Plots: the plots for model accuracy and losses have been plotted and explained above with the code.\n",
    "     e. Code evaluation: The code has been split into 90-10 ratio for comparision. The training data has been tested on various hyperparameters and best results ahve been calculated. The above calculated results are the best results, hence have been add to the report all the others had lower accuracy for all the parameters other than the ones stated. The accuracy obtained for training was maximum for the uncommented network and was equal to 97% on training with test accuracy as 90.72%. For other combinations, training accuracy was 97%and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "asg3 part b.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
